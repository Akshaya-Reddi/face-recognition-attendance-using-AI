# -*- coding: utf-8 -*-
"""Attendance_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jo5gXK0wOldsojS8hN_QiuieGypzBlrM

#Installing all packages
"""

!pip install mtcnn facenet-pytorch gspread oauth2client opencv-python-headless



"""#Extracting Zip File"""

import zipfile
import os

zip_path = "/content/Dataset.zip"  # change if your zip has a different name
extract_dir = "/content/faces"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

print("Extraction complete. Directory structure:")
print(os.listdir(extract_dir))

"""#MTCNN Model"""

from mtcnn import MTCNN
from PIL import Image
import numpy as np
import os

# Initialize the detector
detector = MTCNN()

# Function to extract and crop face
def extract_face(img_path, required_size=(160, 160)):
    image = Image.open(img_path).convert('RGB')
    pixels = np.asarray(image)
    results = detector.detect_faces(pixels)
    if results:
        x1, y1, w, h = results[0]['box']
        x1, y1 = abs(x1), abs(y1)
        x2, y2 = x1 + w, y1 + h
        face = pixels[y1:y2, x1:x2]
        image = Image.fromarray(face).resize(required_size)
        return np.asarray(image)
    else:
        return None

# Adjusted path to the dataset folder
data_dir = "/content/faces/Dataset "

X, y = [], []

for student_id in os.listdir(data_dir):
    student_folder = os.path.join(data_dir, student_id)
    if os.path.isdir(student_folder):
        for filename in os.listdir(student_folder):
            img_path = os.path.join(student_folder, filename)
            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):  # only image files
                try:
                    face = extract_face(img_path)
                    if face is not None:
                        X.append(face)
                        y.append(student_id)
                except Exception as e:
                    print(f"Error processing {img_path}: {e}")

print(f"Dataset created successfully with {len(X)} face images from {len(set(y))} students.")

np.save("faces_X.npy", X)
np.save("faces_y.npy", y)
print("Saved 'faces_X.npy' and 'faces_y.npy'")

from google.colab import files
files.download("faces_X.npy")
files.download("faces_y.npy")

import numpy as np

# Load the .npy file
arr = np.load('faces_X.npy')

# Print the loaded array
print(arr)

"""#Face-Net Model"""

!pip install keras-facenet

import numpy as np
from keras_facenet import FaceNet
embedder = FaceNet()


# Make sure X is a numpy array of faces (shape: num_images x 160 x 160 x 3)
X = np.array(X)

# Generate embeddings for all faces
embeddings = embedder.embeddings(X)

print("Face embeddings generated successfully!")
print("Embeddings shape:", embeddings.shape)

"""#Splitting dataset"""

from sklearn.model_selection import train_test_split

# Step 1: First split into train and temp (train + temp)
X_train, X_temp, y_train, y_temp = train_test_split(
    embeddings, y, test_size=0.3, random_state=42, stratify=y
)

# Step 2: Split the temp set into validation and test
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)

print("Dataset split completed!")
print("Training samples:", X_train.shape[0])
print("Validation samples:", X_val.shape[0])
print("Testing samples:", X_test.shape[0])

"""#Training SVM model"""

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# Initialize SVM with probability estimates enabled
model = SVC(kernel='linear', probability=True)

# Train the model
model.fit(X_train, y_train)

# Evaluate on validation set
y_val_pred = model.predict(X_val)
val_accuracy = accuracy_score(y_val, y_val_pred)

print("Model trained successfully!")
print(f"Validation Accuracy: {val_accuracy * 100:.2f}%")

import joblib

# Save the trained SVM model
joblib.dump(model, 'face_recognition_model.pkl')
print("SVM Model saved successfully!")

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Predict on test set
y_test_pred = model.predict(X_test)

# Calculate accuracy
test_accuracy = accuracy_score(y_test, y_test_pred)

print("Model Evaluation Completed!")
print(f"Test Accuracy: {test_accuracy * 100:.2f}%\n")

# Detailed report
print("Classification Report:")
print(classification_report(y_test, y_test_pred))

# Optional: confusion matrix
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(7,5))
sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix (Test Set)")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

from sklearn.preprocessing import LabelEncoder
import os
import pickle

dataset_path = '/content/faces/Dataset '
labels = os.listdir(dataset_path)

label_encoder = LabelEncoder()
label_encoder.fit(labels)

with open("label_encoder.pkl", "wb") as f:
    pickle.dump(label_encoder, f)

print("Label encoder regenerated successfully.")

"""#Integrating Models"""

import cv2
import numpy as np
from keras_facenet import FaceNet
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import load_model
import datetime
import gspread
from google.oauth2.service_account import Credentials
import os
from google.colab.patches import cv2_imshow

#Load model and encoder
import joblib
model = joblib.load("face_recognition_model.pkl")

# Recreate label encoder
dataset_path = '/content/faces/Dataset '
labels = os.listdir(dataset_path)
label_encoder = LabelEncoder()
label_encoder.fit(labels)

# Initialize FaceNet embedder
embedder = FaceNet()

creds = Credentials.from_service_account_file(
    '/content/credentials.json',
    scopes=['https://www.googleapis.com/auth/spreadsheets']
)

gc = gspread.authorize(creds)

# Replace with your actual Google Sheet ID
SPREADSHEET_ID = "1c74jySP2ATL5ZUWEDhKecrr6IR4rh7TbPDz4ePALtSY"
sheet = gc.open_by_key(SPREADSHEET_ID).sheet1

#Attendance function
def mark_attendance(student_id):
    today = datetime.date.today().strftime("%Y-%m-%d")
    now = datetime.datetime.now().strftime("%H:%M:%S")

    # Fetch all existing records
    records = sheet.get_all_records()

    # Avoid duplicate marking
    for record in records:
        if record['Student ID'] == student_id and record['Date'] == today:
            print(f"Attendance already marked for {student_id} today.")
            return

    # Append a new record
    sheet.append_row([student_id, student_id, today, now, "Present"])
    print(f"Attendance marked for {student_id} at {now}")

from keras_facenet import FaceNet
import numpy as np

embedder = FaceNet()

# Load your saved arrays
X_faces = np.load('faces_X.npy')
y = np.load('faces_y.npy')

# Generate embeddings for all faces
print("Generating embeddings...")
X_embeddings = embedder.embeddings(X_faces)

print(f"Embeddings generated successfully! Shape: {X_embeddings.shape}")
np.save('faces_embeddings.npy', X_embeddings)  # Save for later use

from sklearn.svm import SVC
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import pickle
import numpy as np

# Load embeddings and labels
X = np.load('faces_embeddings.npy')
y = np.load('faces_y.npy')

# Encode labels
label_encoder = LabelEncoder()
y_enc = label_encoder.fit_transform(y)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y_enc, test_size=0.2, random_state=42)

# Train SVM
model = SVC(kernel='linear', probability=True)
model.fit(X_train, y_train)

# Save model and encoder
with open("face_recognition_svm.pkl", "wb") as f:
    pickle.dump(model, f)

with open("label_encoder.pkl", "wb") as f:
    pickle.dump(label_encoder, f)

print("SVM model and label encoder trained and saved successfully!")

!ls -lh | grep svm

"""#Connecting to Google spread sheets"""

import gspread
from google.oauth2.service_account import Credentials

# Google Sheet setup
scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
creds = Credentials.from_service_account_file("credentials.json", scopes=scope)
client = gspread.authorize(creds)

spreadsheet_name = "Attendance_Record"

# Try to open or create the sheet safely
try:
    sheet = client.open(spreadsheet_name).sheet1
    print("Existing sheet found and opened!")
except gspread.SpreadsheetNotFound:
    print("Sheet not found â€” creating a new one...")
    sheet = client.create(spreadsheet_name).sheet1
    sheet.append_row(["Student ID", "Date", "Time", "Status"])
    print("New Google Sheet created and initialized!")

# Verify connection
print(f"Connected to: {spreadsheet_name}")



!pip install flask pyngrok qrcode[pil]

from pyngrok import ngrok

# Connect your ngrok account (paste your token inside quotes)
!ngrok config add-authtoken "34itkIUzqRNUWZdYSm0wTuqwAfd_6dh2BjN3ZEtLJC69L5ppd"

# Create a public URL for the Flask app
public_url = ngrok.connect(5000).public_url
print("Public URL:", public_url)

!pip install keras-facenet

from keras_facenet import FaceNet
print("FaceNet installed successfully!")

!ls -lh

# Imports
from flask import Flask, request, render_template_string
import qrcode
from pyngrok import ngrok
import cv2
import numpy as np
import pickle
import os
from keras_facenet import FaceNet
from mtcnn import MTCNN
from sklearn.preprocessing import LabelEncoder
from google.oauth2.service_account import Credentials
import gspread
from datetime import datetime
from threading import Thread

#Initialize Flask
app = Flask(__name__)

#Load trained model
print("Loading FaceNet and MTCNN...")
embedder = FaceNet()
detector = MTCNN()

print("Loading trained model (SVM)...")
with open("face_recognition_svm.pkl", "rb") as f:
    model = pickle.load(f)

#Label Encoder (auto from dataset folder)
dataset_path = "/content/faces/Dataset "  # fixed extra space
if not os.path.exists(dataset_path):
    raise FileNotFoundError(f"Dataset path not found: {dataset_path}")

labels = sorted(os.listdir(dataset_path))
label_encoder = LabelEncoder()
label_encoder.fit(labels)

# Google Sheets Setup
print("Connecting to Google Sheets...")
scope = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive"
]
creds = Credentials.from_service_account_file("credentials.json", scopes=scope)
client = gspread.authorize(creds)
sheet = client.open("Attendance_Record").sheet1

#Attendance Marking
def mark_attendance(student_id):
    date = datetime.now().strftime("%Y-%m-%d")
    time = datetime.now().strftime("%H:%M:%S")
    sheet.append_row([student_id, date, time, "Present"])
    print(f"Attendance marked for {student_id}")

#HTML Page
UPLOAD_PAGE = """
<!DOCTYPE html>
<html>
  <head>
    <title>Face Attendance</title>
    <style>
      body {
        font-family: Arial;
        text-align: center;
        margin-top: 30px;
        background-color: #f4f4f9;
      }
      video, canvas {
        border: 2px solid #444;
        border-radius: 10px;
        margin: 10px;
      }
      button {
        padding: 10px 20px;
        font-size: 16px;
        background-color: #007bff;
        color: white;
        border: none;
        border-radius: 8px;
        cursor: pointer;
      }
      button:hover {
        background-color: #0056b3;
      }
    </style>
  </head>
  <body>
    <h2>Face Attendance System</h2>
    <p>Allow camera access and click "Capture & Submit"</p>

    <video id="video" width="300" height="250" autoplay></video><br>
    <button id="capture">Capture & Submit</button>
    <canvas id="canvas" width="300" height="250" style="display:none;"></canvas>

    <p id="status"></p>

    <script>
      const video = document.getElementById('video');
      const canvas = document.getElementById('canvas');
      const captureBtn = document.getElementById('capture');
      const statusText = document.getElementById('status');

      // Start camera
      navigator.mediaDevices.getUserMedia({ video: true })
        .then(stream => { video.srcObject = stream; })
        .catch(err => { alert("Camera access denied: " + err); });

      // Capture frame and send to Flask
      captureBtn.addEventListener('click', async () => {
        const context = canvas.getContext('2d');
        context.drawImage(video, 0, 0, canvas.width, canvas.height);

        const dataUrl = canvas.toDataURL('image/jpeg');
        statusText.innerText = "Uploading...";

  const response = await fetch('/upload', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ image: dataUrl })
});

const result = await response.json();
statusText.innerText = result.message || result.error;

      });
    </script>
  </body>
</html>
"""


# Routes
@app.route('/')
def index():
    return render_template_string(UPLOAD_PAGE)

@app.route('/upload', methods=['POST'])
def upload():
    file = request.files['image']
    path = "temp.jpg"
    file.save(path)

    #  Face Recognition
    img = cv2.imread(path)
    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    faces = detector.detect_faces(rgb)

    if len(faces) == 0:
        return "No face detected. Try again."

    x, y, w, h = faces[0]['box']
    face_img = rgb[y:y+h, x:x+w]
    face_img = cv2.resize(face_img, (160, 160))
    face_img = np.expand_dims(face_img, axis=0)

    # Generate embedding
    embedding = embedder.embeddings(face_img)

    # Get prediction and confidence
    probs = model.predict_proba(embedding)
    confidence = np.max(probs)
    predicted_class = np.argmax(probs)
    name = label_encoder.inverse_transform([predicted_class])[0]

    # Confidence Check
    if confidence < 0.70:
        print(f"Unknown face detected (confidence: {confidence:.2f})")
        return "Student Not Found (Low Confidence)"

    # Mark attendance
    mark_attendance(name)
    print(f"Attendance marked for {name} ({confidence:.2f})")
    return f"Attendance marked successfully for {name}!"


# Flask Server Thread
def run_app():
    app.run(port=7000)

# Ngrok Tunnel
public_url = ngrok.connect(7000)
print(f"Public URL: {public_url}")
print("Flask app is running...")

Thread(target=run_app).start()

# Imports
from flask import Flask, request, render_template_string, jsonify
from pyngrok import ngrok
import cv2
import numpy as np
import pickle
import os
from keras_facenet import FaceNet
from mtcnn import MTCNN
from sklearn.preprocessing import LabelEncoder
from google.oauth2.service_account import Credentials
import gspread
from datetime import datetime
from threading import Thread
import base64
import re
import qrcode
from IPython.display import Image, display

# Initialize Flask
app = Flask(__name__)

# Load trained model
print(" Loading FaceNet and MTCNN...")
embedder = FaceNet()
detector = MTCNN()

print(" Loading trained model (SVM)...")
with open("face_recognition_svm.pkl", "rb") as f:
    model = pickle.load(f)

# Label Encoder (auto from dataset folder)
dataset_path = "/content/faces/Dataset "
if not os.path.exists(dataset_path):
    raise FileNotFoundError(f"Dataset path not found: {dataset_path}")

labels = sorted(os.listdir(dataset_path))
label_encoder = LabelEncoder()
label_encoder.fit(labels)

# Google Sheets Setup
print(" Connecting to Google Sheets...")
scope = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive"
]
creds = Credentials.from_service_account_file("credentials.json", scopes=scope)
client = gspread.authorize(creds)
sheet = client.open("Attendance_Record").sheet1


# Attendance Marking
def mark_attendance(student_id):
    date = datetime.now().strftime("%Y-%m-%d")
    time = datetime.now().strftime("%H:%M:%S")
    sheet.append_row([student_id, date, time, "Present"])
    print(f"Attendance marked for {student_id}")


# HTML (Camera interface)
HTML_PAGE = """
<!DOCTYPE html>
<html>
<head>
  <title>Face Attendance</title>
  <style>
    body { font-family: Arial; text-align: center; background: #f4f4f9; margin-top: 40px; }
    video, canvas { border: 2px solid #555; border-radius: 10px; margin: 10px; }
    button {
      padding: 10px 20px; font-size: 16px; border: none; border-radius: 8px;
      background-color: #007bff; color: white; cursor: pointer;
    }
    button:hover { background-color: #0056b3; }
  </style>
</head>
<body>
  <h2>ðŸŽ“ Face Attendance System</h2>
  <p>Allow camera access and click "Capture & Submit"</p>

  <video id="video" width="300" height="250" autoplay></video><br>
  <button id="capture">Capture & Submit</button>
  <canvas id="canvas" width="300" height="250" style="display:none;"></canvas>

  <p id="status"></p>

  <script>
    const video = document.getElementById('video');
    const canvas = document.getElementById('canvas');
    const captureBtn = document.getElementById('capture');
    const statusText = document.getElementById('status');

    // Access camera
    navigator.mediaDevices.getUserMedia({ video: true })
      .then(stream => { video.srcObject = stream; })
      .catch(err => { alert("Camera access denied: " + err); });

    // Capture and send
    captureBtn.addEventListener('click', async () => {
      const context = canvas.getContext('2d');
      context.drawImage(video, 0, 0, canvas.width, canvas.height);
      const dataUrl = canvas.toDataURL('image/jpeg');

      statusText.innerText = "Processing...";
      const response = await fetch('/upload', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ image: dataUrl })
      });

      const result = await response.json();
      statusText.innerText = result.message;
    });
  </script>
</body>
</html>
"""

# Flask Routes
@app.route('/')
def index():
    return render_template_string(HTML_PAGE)


@app.route('/upload', methods=['POST'])
def upload():
    data = request.get_json()
    image_data = data.get('image')

    # Decode base64 image
    image_data = re.sub('^data:image/.+;base64,', '', image_data)
    img_bytes = base64.b64decode(image_data)
    np_img = np.frombuffer(img_bytes, np.uint8)
    img = cv2.imdecode(np_img, cv2.IMREAD_COLOR)

    # Detect faces
    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    faces = detector.detect_faces(rgb)

    if len(faces) == 0:
        return jsonify({"message": "No face detected. Try again."})
    elif len(faces) > 1:
        return jsonify({"message": "Multiple faces detected! Please show only one face."})

    # Process single detected face
    x, y, w, h = faces[0]['box']
    face_img = rgb[y:y+h, x:x+w]
    face_img = cv2.resize(face_img, (160, 160))
    face_img = np.expand_dims(face_img, axis=0)

    embedding = embedder.embeddings(face_img)
    probs = model.predict_proba(embedding)
    confidence = np.max(probs)
    predicted_class = np.argmax(probs)
    name = label_encoder.inverse_transform([predicted_class])[0]

    # Confidence threshold check
    if confidence < 0.70:
        print(f"Unknown face detected (confidence={confidence:.2f})")
        return jsonify({"message": "Face not matched with any student."})

    # Mark attendance for valid face
    mark_attendance(name)
    print(f"Attendance marked for {name} ({confidence:.2f})")
    return jsonify({"message": f"Attendance marked successfully for {name}!"})


# Run Flask + Ngrok
def run_app():
    app.run(port=4000)

public_url = ngrok.connect(4000)
print(f"Public URL: {public_url}")
print("Flask app is running...")

# Generate and display QR code
qr = qrcode.make(str(public_url))
qr.save("attendance_qr.png")
display(Image(filename="attendance_qr.png"))

Thread(target=run_app).start()



